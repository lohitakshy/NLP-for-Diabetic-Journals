{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gensim accomplishes this by taking a corpus, a collection of text documents, and producing a vector representation of\n",
    "#the text in the corpus. The vector representation can then be used to train a model, which is an algorithm to create \n",
    "#different representations of the data, which are usually more semantic. These three concepts are key to understanding \n",
    "#how Gensim works. At the same time, we'll work through a simple example that illustrates each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim Small example \n",
    "# Small corpus for this example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A corpus is a collection of digital documents. This corpus is fed to Gensim from which it will infer the structure of the \n",
    "#documents and extract topics from the documents. Once the algorithm learns on how to infer topics from the training corpus, \n",
    "#it can be used to assign topics to new documents which were not present in the training corpus.For this reason, we also refer\n",
    "#to this collection as the training corpus. No human intervention is required - the topic classification is unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this portion we are reading whole corpus and making this in one whole big corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using glob we can read and write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import glob \n",
    "\n",
    "a = open(\"output.txt\",'a',encoding=\"utf-8\")\n",
    "counter = 0\n",
    "#var = \"\"\n",
    "list_of_files = glob.glob(r\"C:\\Users\\lohit\\Desktop\\course\\KDD\\FinalProject\\diabetes\\diabetes\\*.txt\")           # create the list of file\n",
    "for file_name in list_of_files:\n",
    "    FI = open(file_name, 'r',encoding=\"utf-8\")\n",
    "    #print(FI.read())\n",
    "  \n",
    "    for line in FI:\n",
    "        \n",
    "        a.write(line)\n",
    "        #print(line)\n",
    "  \n",
    "    counter = counter + 1\n",
    "  \n",
    "  #for line in FI:\n",
    "   # print(line)\n",
    "\n",
    "  #print(FI)\n",
    "FI.close()\n",
    "#FO.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We first remove all the words the commonly used English words - called stop words such as 'the', ‘a’, ‘we’, etc.) and words \n",
    "# that occur only once in the corpus.\n",
    "# Second we are counting number of different words inside the document and if the count is more than one it will keep on adding\n",
    "# frequency and calcuate the total frequency of all words.\n",
    "# Now we are take the words which are more than 0 time inside the corpus.\n",
    "# After that we are printing the words.\n",
    "# Using NLTK library we are calculating the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#word_tokenize accepts a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(r\"C:\\Users\\lohit\\Desktop\\course\\KDD\\output.txt\",'r',encoding='utf-8')\n",
    "line = file1.read()# Use this to read file content as a stream:\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a',encoding='utf-8')\n",
    "        appendFile.write(\" \"+r)\n",
    "        \n",
    "appendFile.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.121928094887362"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_text = open(r\"C:\\Users\\lohit\\Desktop\\course\\KDD\\out.txt\",'r')\n",
    "x = document_text.read()\n",
    "import collections\n",
    "import math\n",
    "def H(s):\n",
    "    probabilities = [n_x/len(s) for x,n_x in collections.Counter(s).items()]\n",
    "    e_x = [-p_x*math.log(p_x,2) for p_x in probabilities]    \n",
    "    return sum(e_x)\n",
    "\n",
    "H(\"testicular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "#frequency = {}\n",
    "document_text = open(r\"C:\\Users\\lohit\\Desktop\\course\\KDD\\out.txt\",'r')\n",
    "text_string = document_text.read().split()\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in text_string:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 0] for text in text_string]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in content]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 0] for text in texts]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We now need to tokenize our data. This breaks the documents into words and assigns tokens: unique numbers to the words that\n",
    "#have been repeated more than “x” times. Thus, we associate each word in the corpus with a unique integer ID. We can do this \n",
    "#using the Gensim.corpora.Dictionary class. This dictionary defines the vocabulary of all words that our processing knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lohit\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(10455 unique tokens: ['c:\\\\users\\\\lohit\\\\desktop\\\\course\\\\kdd\\\\finalproject\\\\diabetes\\\\diabetes\\\\100032.txt', 'c:\\\\users\\\\lohit\\\\desktop\\\\course\\\\kdd\\\\finalproject\\\\diabetes\\\\diabetes\\\\100065.txt', 'c:\\\\users\\\\lohit\\\\desktop\\\\course\\\\kdd\\\\finalproject\\\\diabetes\\\\diabetes\\\\100079.txt', 'c:\\\\users\\\\lohit\\\\desktop\\\\course\\\\kdd\\\\finalproject\\\\diabetes\\\\diabetes\\\\100082.txt', 'c:\\\\users\\\\lohit\\\\desktop\\\\course\\\\kdd\\\\finalproject\\\\diabetes\\\\diabetes\\\\100108.txt']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "dictionary.save('C:/Users/lohit/AppData/Local/Temp/den.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next, we need to represent the documents mathematically to be able to continue further processing, so we represent each \n",
    "#document as a vector. We use the bag-of-words model where each document is represented by a vector containing the frequency \n",
    "#counts of each word in the dictionary. The length of the vector is the number of entries in the dictionary. One of the main \n",
    "#properties of the bag-of-words model is that it completely ignores the order of the tokens in the document that is encoded,\n",
    "#hence bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human': 0, 'machine': 1, 'interface': 2, 'lab': 3, 'abc': 4, 'computer': 5, 'applications': 6, 'survey': 7, 'user': 8, 'opinion': 9, 'system': 10, 'response': 11, 'time': 12, 'eps': 13, 'management': 14, 'engineering': 15, 'testing': 16, 'relation': 17, 'perceived': 18, 'error': 19, 'measurement': 20, 'generation': 21, 'random': 22, 'binary': 23, 'unordered': 24, 'trees': 25, 'intersection': 26, 'graph': 27, 'paths': 28, 'minors': 29, 'iv': 30, 'widths': 31, 'well': 32, 'quasi': 33, 'ordering': 34}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now are trying some small example and make vector for them to get the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"found when among transfer saliva\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (13, 1), (17, 1)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"EPS computer relation\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count\n",
    "#of this token. Now changing the whole courpus into vector form.\n",
    "#We are saving this into our temporary folder called with an extension of “.mm”. Note that while this list lives entirely\n",
    "#in memory, in most applications you will want a more scalable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:31,112 : INFO : storing corpus in Matrix Market format to C:/Users/lohit/AppData/Local/Temp/den.mm\n",
      "2017-10-25 18:24:31,116 : INFO : saving sparse matrix to C:/Users/lohit/AppData/Local/Temp/den.mm\n",
      "2017-10-25 18:24:31,118 : INFO : PROGRESS: saving document #0\n",
      "2017-10-25 18:24:31,121 : INFO : saved 9x35 matrix, density=16.190% (51/315)\n",
      "2017-10-25 18:24:31,126 : INFO : saving MmCorpus index to C:/Users/lohit/AppData/Local/Temp/den.mm.index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(5, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(2, 1), (8, 1), (10, 1), (13, 1), (14, 1)],\n",
       " [(0, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
       " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(25, 1), (26, 1), (27, 1), (28, 1)],\n",
       " [(25, 1), (27, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
       " [(7, 1), (27, 1), (29, 1)]]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "corpora.MmCorpus.serialize('C:/Users/lohit/AppData/Local/Temp/den.mm', bow_corpus)\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term \n",
    "#referring to a transformation from one document representation to another. In Gensim documents are represented as vectors\n",
    "#so a model can be thought of as a transformation between two vector spaces. The details of this transformation are learned \n",
    "#from the training corpus. One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words\n",
    "#representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in \n",
    "#the corpus.\n",
    "#Term Frequency, Inverse Document Frequency (TF-IDF) TF-IDF is a way to score the importance of words (or \"terms\") in a \n",
    "#document based on how frequently they appear across multiple documents.\n",
    "#If a word appears frequently in a document, it's important and TF-IDF gives the word a high score.\n",
    "#But if a word appears in many documents, then it's not a unique identifier and gives the word a low score.\n",
    "#Therefore, stop words will be scaled down. Words that appear frequently in a single document will be scaled up.\n",
    "#For a term t in a document d, the weight Wt, d of term t in document d is given by:\n",
    "#Wt,d = TFt,d log (N/DFt )\n",
    "#Where,\n",
    "#TFt,d is the number of occurrences of t in document d.\n",
    "#DFt is the number of documents containing term t.\n",
    "#N is the total number of documents in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:32,547 : INFO : collecting document frequencies\n",
      "2017-10-25 18:24:32,549 : INFO : PROGRESS: processing document #0\n",
      "2017-10-25 18:24:32,551 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.5773502691896257), (5, 0.5773502691896257), (13, 0.5773502691896257)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "# transform the \"system minors\" string\n",
    "tfidf[dictionary.doc2bow(\"human computer interaction EPS\".lower().split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The tfidf model again returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf\n",
    "#weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\lohit\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os.path\n",
    "\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:35,514 : INFO : loading Dictionary object from C:\\Users\\lohit\\AppData\\Local\\Temp\\den.dict\n",
      "2017-10-25 18:24:35,519 : INFO : loaded C:\\Users\\lohit\\AppData\\Local\\Temp\\den.dict\n",
      "2017-10-25 18:24:35,522 : INFO : loaded corpus index from C:\\Users\\lohit\\AppData\\Local\\Temp\\den.mm.index\n",
      "2017-10-25 18:24:35,525 : INFO : initializing corpus reader from C:\\Users\\lohit\\AppData\\Local\\Temp\\den.mm\n",
      "2017-10-25 18:24:35,529 : INFO : accepted corpus with 9 documents, 35 features, 51 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated before \n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "if os.path.isfile(os.path.join(TEMP_FOLDER, 'den.dict')):\n",
    "    dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'den.dict'))\n",
    "    corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'den.mm'))\n",
    "    print(\"Used files generated before \")\n",
    "else:\n",
    "    print(\"Run again error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "machine\n",
      "interface\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[0])\n",
    "print(dictionary[1])\n",
    "print(dictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:37,005 : INFO : collecting document frequencies\n",
      "2017-10-25 18:24:37,012 : INFO : PROGRESS: processing document #0\n",
      "2017-10-25 18:24:37,014 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5648663441460566), (1, 0.8251824121072071)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5245699338309155), (1, 0.38315779281548723), (2, 0.2622849669154578), (3, 0.38315779281548723), (4, 0.38315779281548723), (5, 0.2622849669154578), (6, 0.38315779281548723)]\n",
      "[(5, 0.3726494271826947), (7, 0.3726494271826947), (8, 0.27219160459794917), (9, 0.5443832091958983), (10, 0.27219160459794917), (11, 0.3726494271826947), (12, 0.3726494271826947)]\n",
      "[(2, 0.438482464916089), (8, 0.32027755044706185), (10, 0.32027755044706185), (13, 0.438482464916089), (14, 0.6405551008941237)]\n",
      "[(0, 0.3449874408519962), (10, 0.5039733231394895), (13, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n",
      "[(8, 0.21953536176370683), (11, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]\n",
      "[(25, 0.31622776601683794), (26, 0.6324555320336759), (27, 0.31622776601683794), (28, 0.6324555320336759)]\n",
      "[(25, 0.20466057569885868), (27, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]\n",
      "[(7, 0.6282580468670046), (27, 0.45889394536615247), (29, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Here we transformed our Tf-Idf corpus via Latent Semantic Indexing into a latent 2-D space (2-D because we set num_topics=2).\n",
    "#Now you’re probably wondering: what do these two latent dimensions stand for? Let’s inspect with models.LsiModel.print_topics():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:39,891 : INFO : using serial LSI version on this node\n",
      "2017-10-25 18:24:39,895 : INFO : updating model with new documents\n",
      "2017-10-25 18:24:39,901 : INFO : preparing a new chunk of documents\n",
      "2017-10-25 18:24:39,904 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-10-25 18:24:39,906 : INFO : 1st phase: constructing (35, 102) action matrix\n",
      "2017-10-25 18:24:39,909 : INFO : orthonormalizing (35, 102) action matrix\n",
      "2017-10-25 18:24:39,916 : INFO : 2nd phase: running dense svd on (35, 9) matrix\n",
      "2017-10-25 18:24:39,920 : INFO : computing the final decomposition\n",
      "2017-10-25 18:24:39,922 : INFO : keeping 2 factors (discarding 66.599% of energy spectrum)\n",
      "2017-10-25 18:24:39,924 : INFO : processed documents up to #9\n",
      "2017-10-25 18:24:39,926 : INFO : topic #0(1.271): 0.408*\"system\" + 0.301*\"survey\" + 0.283*\"user\" + 0.282*\"eps\" + 0.246*\"human\" + 0.236*\"management\" + 0.227*\"opinion\" + 0.226*\"response\" + 0.226*\"time\" + 0.224*\"interface\"\n",
      "2017-10-25 18:24:39,928 : INFO : topic #1(1.180): 0.425*\"minors\" + 0.422*\"graph\" + 0.313*\"survey\" + 0.236*\"trees\" + 0.222*\"intersection\" + 0.222*\"paths\" + 0.188*\"widths\" + 0.188*\"ordering\" + 0.188*\"quasi\" + 0.188*\"well\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the topics are printed to log – see the note at the top of this page about activating logging\n",
    "#It appears that according to LSI, “trees”, “graph” and “minors” are all related words (and contribute the most to the direction\n",
    "#of the first topic), while the second topic practically concerns itself with all the other words. As expected,\n",
    "#the first five documents are more strongly related to the second topic while the remaining four documents to the first topic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:42,179 : INFO : topic #0(1.271): 0.408*\"system\" + 0.301*\"survey\" + 0.283*\"user\" + 0.282*\"eps\" + 0.246*\"human\" + 0.236*\"management\" + 0.227*\"opinion\" + 0.226*\"response\" + 0.226*\"time\" + 0.224*\"interface\"\n",
      "2017-10-25 18:24:42,185 : INFO : topic #1(1.180): 0.425*\"minors\" + 0.422*\"graph\" + 0.313*\"survey\" + 0.236*\"trees\" + 0.222*\"intersection\" + 0.222*\"paths\" + 0.188*\"widths\" + 0.188*\"ordering\" + 0.188*\"quasi\" + 0.188*\"well\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.408*\"system\" + 0.301*\"survey\" + 0.283*\"user\" + 0.282*\"eps\" + 0.246*\"human\" + 0.236*\"management\" + 0.227*\"opinion\" + 0.226*\"response\" + 0.226*\"time\" + 0.224*\"interface\"'),\n",
       " (1,\n",
       "  '0.425*\"minors\" + 0.422*\"graph\" + 0.313*\"survey\" + 0.236*\"trees\" + 0.222*\"intersection\" + 0.222*\"paths\" + 0.188*\"widths\" + 0.188*\"ordering\" + 0.188*\"quasi\" + 0.188*\"well\"')]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.38491889088288589), (1, -0.23799083182688749)]\n",
      "[(0, 0.67327570398106373), (1, 0.061063011983416668)]\n",
      "[(0, 0.59429153367225518), (1, -0.32013947136769177)]\n",
      "[(0, 0.56595474120106282), (1, -0.3443358962412072)]\n",
      "[(0, 0.37883572265509352), (1, -0.013238424814016938)]\n",
      "[(0, 0.032346275240837975), (1, 0.17672649803391668)]\n",
      "[(0, 0.13320822144013733), (1, 0.48928974583068763)]\n",
      "[(0, 0.19468626669806066), (1, 0.6377245136397599)]\n",
      "[(0, 0.37343003126004548), (1, 0.65767275604411801)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:43,985 : INFO : saving Projection object under C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi.projection, separately None\n",
      "2017-10-25 18:24:44,000 : INFO : saved C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi.projection\n",
      "2017-10-25 18:24:44,002 : INFO : saving LsiModel object under C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi, separately None\n",
      "2017-10-25 18:24:44,005 : INFO : not storing attribute projection\n",
      "2017-10-25 18:24:44,007 : INFO : not storing attribute dispatcher\n",
      "2017-10-25 18:24:44,012 : INFO : saved C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi\n"
     ]
    }
   ],
   "source": [
    "lsi.save(os.path.join(TEMP_FOLDER, 'model.lsi')) # same for tfidf, lda, ...\n",
    "#lsi = models.LsiModel.load(os.path.join(TEMP_FOLDER, 'model.lsi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A common reason for such a charade is that we want to determine similarity between pairs of documents, or the similarity\n",
    "#between a specific document and a set of other documents (such as a user query vs. indexed documents).\n",
    "#To show how this can be done in gensim,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:45,463 : INFO : loading Dictionary object from C:/Users/lohit/AppData/Local/Temp/den.dict\n",
      "2017-10-25 18:24:45,467 : INFO : loaded C:/Users/lohit/AppData/Local/Temp/den.dict\n",
      "2017-10-25 18:24:45,470 : INFO : loaded corpus index from C:/Users/lohit/AppData/Local/Temp/den.mm.index\n",
      "2017-10-25 18:24:45,473 : INFO : initializing corpus reader from C:/Users/lohit/AppData/Local/Temp/den.mm\n",
      "2017-10-25 18:24:45,475 : INFO : accepted corpus with 9 documents, 35 features, 51 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 35 features, 51 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('C:/Users/lohit/AppData/Local/Temp/den.dict')\n",
    "corpus = corpora.MmCorpus('C:/Users/lohit/AppData/Local/Temp/den.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now suppose a user typed in the query “Human computer interaction”. We would like to sort our nine corpus documents in \n",
    "#decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of\n",
    "#possible similarities—on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks,\n",
    "#just a semantic extension over the boolean keyword match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46390054531878433), (1, -0.20359920281643867)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In addition, we will be considering cosine similarity to determine the similarity of two vectors. Cosine similarity is a\n",
    "#standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity \n",
    "#measures may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing query structures\n",
    "#To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. \n",
    "#In our case, they are the same nine documents used for training LSI, converted to 2-D LSA space. But that’s only incidental, \n",
    "#we might also be indexing a different corpus altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:49,074 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-10-25 18:24:49,087 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Index persistency is handled via the standard save() and load() functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:50,465 : INFO : saving MatrixSimilarity object under C:/Users/lohit/AppData/Local/Temp/den.index, separately None\n",
      "2017-10-25 18:24:50,472 : INFO : saved C:/Users/lohit/AppData/Local/Temp/den.index\n",
      "2017-10-25 18:24:50,474 : INFO : loading MatrixSimilarity object from C:/Users/lohit/AppData/Local/Temp/den.index\n",
      "2017-10-25 18:24:50,478 : INFO : loaded C:/Users/lohit/AppData/Local/Temp/den.index\n"
     ]
    }
   ],
   "source": [
    "index.save('C:/Users/lohit/AppData/Local/Temp/den.index')\n",
    "index = similarities.MatrixSimilarity.load('C:/Users/lohit/AppData/Local/Temp/den.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is true for all similarity indexing classes (similarities.Similarity, similarities.MatrixSimilarity and\n",
    "#similarities.SparseMatrixSimilarity). Also in the following, index can be an object of any of these. When in doubt, \n",
    "#use similarities.Similarity, as it is the most scalable version, and it also supports adding more documents to the index later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99145329), (1, 0.8966043), (2, 0.99835241), (3, 0.99378079), (4, 0.9334408), (5, -0.21829586), (6, -0.13329136), (7, -0.10740998), (8, 0.08808583)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar), so that the first document has a\n",
    "#score of 0.99809301 etc.\n",
    "#With some standard Python magic we sort these similarities into descending order, and obtain the final answer to the query \n",
    "#“Human computer interaction”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.99835241), (3, 0.99378079), (0, 0.99145329), (4, 0.9334408), (1, 0.8966043), (8, 0.08808583), (7, -0.10740998), (6, -0.13329136), (5, -0.21829586)]\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The thing to note here is that documents no. 2 (\"The EPS user interface management system\") and\n",
    "#4 (\"Relation of user perceived response time to error measurement\") would never be returned by a standard boolean fulltext\n",
    "#search, because they do not share any common words with \"Human computer interaction\". \n",
    "#However, after applying LSI, we can observe that both of them received quite high similarity scores \n",
    "#(no. 2 is actually the most similar!), which corresponds better to our intuition of them sharing a “computer-human” \n",
    "#related topic with the query. In fact, this semantic generalization is the reason why we apply transformations and do\n",
    "#topic modelling in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
